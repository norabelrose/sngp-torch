{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.datasets import load_diabetes\n",
    "import torch\n",
    "\n",
    "ds: Any = load_diabetes()\n",
    "X = torch.from_numpy(ds['data']).float()\n",
    "y = torch.from_numpy(ds['target']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.linalg_eigh(\n",
      "eigenvalues=tensor([0.3373, 0.9798, 0.9928, 0.9949, 0.9960, 0.9971, 0.9974, 0.9981, 0.9984,\n",
      "        0.9997, 0.9999, 0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000]),\n",
      "eigenvectors=tensor([[ 4.5312e-02,  3.2639e-02,  2.8694e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.0633e-01,  2.4871e-02,  1.2039e-02,  ...,  4.3271e-05,\n",
      "          8.4106e-02,  1.6835e-01],\n",
      "        [ 4.3281e-02, -1.9225e-01, -9.1757e-02,  ..., -3.2827e-02,\n",
      "         -3.3616e-02, -8.4996e-02],\n",
      "        ...,\n",
      "        [ 1.1421e-01,  5.4737e-02,  4.4006e-02,  ...,  5.7645e-02,\n",
      "          1.0546e-02, -5.3869e-03],\n",
      "        [ 7.9995e-02, -2.5983e-02, -9.6555e-02,  ..., -5.7589e-02,\n",
      "          1.1835e-01, -2.7000e-02],\n",
      "        [ 2.6090e-02,  8.4215e-02,  4.9238e-03,  ..., -8.7870e-02,\n",
      "          2.8486e-02, -7.6764e-02]]))\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "torch.linalg_cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 56 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/nora/Code/sngp-torch/examples/synthetic_data.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nora/Code/sngp-torch/examples/synthetic_data.ipynb#ch0000003?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m gp\u001b[39m.\u001b[39mrecord_covariance():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nora/Code/sngp-torch/examples/synthetic_data.ipynb#ch0000003?line=17'>18</a>\u001b[0m     gp(X)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nora/Code/sngp-torch/examples/synthetic_data.ipynb#ch0000003?line=19'>20</a>\u001b[0m gp\u001b[39m.\u001b[39;49mposterior(X)\n",
      "File \u001b[0;32m~/Code/sngp-torch/sngp_torch/random_feature_gp.py:104\u001b[0m, in \u001b[0;36mRandomFeatureGP.posterior\u001b[0;34m(self, hiddens, diag_only)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nora/Code/sngp-torch/sngp_torch/random_feature_gp.py?line=101'>102</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/nora/Code/sngp-torch/sngp_torch/random_feature_gp.py?line=102'>103</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovariance_matrix)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/nora/Code/sngp-torch/sngp_torch/random_feature_gp.py?line=103'>104</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m MultivariateNormal(mean, phi \u001b[39m@\u001b[39;49m prod, validate_args\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:151\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nora/miniconda3/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py?line=148'>149</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m scale_tril\n\u001b[1;32m    <a href='file:///home/nora/miniconda3/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py?line=149'>150</a>\u001b[0m \u001b[39melif\u001b[39;00m covariance_matrix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/nora/miniconda3/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py?line=150'>151</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mcholesky(covariance_matrix)\n\u001b[1;32m    <a href='file:///home/nora/miniconda3/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py?line=151'>152</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# precision_matrix is not None\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/nora/miniconda3/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py?line=152'>153</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m _precision_to_scale_tril(precision_matrix)\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: torch.linalg_cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 56 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "from sngp_torch import RandomFeatureGP\n",
    "from torch.optim import LBFGS\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "gp = RandomFeatureGP(X.shape[1], 1, 'mse', num_rff=128)\n",
    "opt = LBFGS(gp.parameters(), line_search_fn='strong_wolfe')\n",
    "\n",
    "def closure():\n",
    "    loss = F.mse_loss(gp(X).squeeze(), y)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "opt.step(closure)\n",
    "opt.zero_grad()\n",
    "\n",
    "with gp.record_covariance():\n",
    "    gp(X)\n",
    "\n",
    "gp.posterior(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([442, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functorch import hessian\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "import torch\n",
    "\n",
    "features = torch.randn(1000, 256).cuda()\n",
    "weights = torch.randn(256).cuda()\n",
    "labels = torch.empty(1000).bernoulli(0.5).cuda()\n",
    "\n",
    "def loss_fn(w):\n",
    "    return binary_cross_entropy_with_logits(torch.squeeze(features @ w[..., None]), labels)\n",
    "\n",
    "# loss = binary_cross_entropy_with_logits(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 µs ± 7.31 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.functional as F\n",
    "\n",
    "%timeit F.hessian(loss_fn, weights, vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5585e-02, -1.0187e-04, -3.9561e-04,  ...,  4.9680e-04,\n",
       "         -5.4930e-06, -1.9473e-04],\n",
       "        [-1.0187e-04,  2.7010e-02, -3.9171e-03,  ...,  3.6581e-03,\n",
       "          8.8218e-04,  1.0315e-03],\n",
       "        [-3.9561e-04, -3.9171e-03,  2.7941e-02,  ..., -4.0300e-04,\n",
       "          1.8802e-03,  1.3838e-03],\n",
       "        ...,\n",
       "        [ 4.9680e-04,  3.6581e-03, -4.0300e-04,  ...,  2.7453e-02,\n",
       "          7.9008e-04,  2.1069e-03],\n",
       "        [-5.4930e-06,  8.8218e-04,  1.8802e-03,  ...,  7.9008e-04,\n",
       "          2.8246e-02, -3.9255e-04],\n",
       "        [-1.9473e-04,  1.0315e-03,  1.3838e-03,  ...,  2.1069e-03,\n",
       "         -3.9255e-04,  2.7641e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.hessian(loss_fn, weights, vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515 µs ± 748 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "foo = torch.rand(512, 512).cuda()\n",
    "foo = foo @ foo.t()\n",
    "%timeit torch.linalg.cholesky_ex(foo, upper=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5585e+01, -1.0187e-01, -3.9561e-01,  ...,  4.9680e-01,\n",
       "         -5.4931e-03, -1.9473e-01],\n",
       "        [-1.0187e-01,  2.7010e+01, -3.9171e+00,  ...,  3.6581e+00,\n",
       "          8.8218e-01,  1.0315e+00],\n",
       "        [-3.9561e-01, -3.9171e+00,  2.7941e+01,  ..., -4.0300e-01,\n",
       "          1.8802e+00,  1.3838e+00],\n",
       "        ...,\n",
       "        [ 4.9680e-01,  3.6581e+00, -4.0300e-01,  ...,  2.7453e+01,\n",
       "          7.9008e-01,  2.1069e+00],\n",
       "        [-5.4931e-03,  8.8218e-01,  1.8802e+00,  ...,  7.9008e-01,\n",
       "          2.8246e+01, -3.9255e-01],\n",
       "        [-1.9473e-01,  1.0315e+00,  1.3838e+00,  ...,  2.1069e+00,\n",
       "         -3.9255e-01,  2.7641e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fast_hessian(w):\n",
    "    logits = features @ w[..., None]\n",
    "    probs = logits.sigmoid()\n",
    "    x = torch.sqrt(probs * (1 - probs)) * features\n",
    "    return x.T @ x\n",
    "\n",
    "fast_hessian(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa81541598839f570aad5dc7c0c2a1b79738f10fbd2c927ab4c8b970fa51d8f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
